%-*- Mode:LaTeX; -*-      
\documentclass[11pt]{article}
\usepackage{vmargin}		% Force narrower margins
\setpapersize{USletter}
\setmarginsrb{1.0in}{1.0in}{1.0in}{0.6in}{0pt}{0pt}{0pt}{0.4in}
\setlength{\parskip}{.1in}  % removed space between paragraphs
\setlength{\parindent}{0in}

\usepackage{epsfig}
\usepackage{graphicx}

\begin{document}

\large
\begin{center}
{\bf CS-5340/6340, Written Assignment \#3} \\
{\bf DUE: Thursday, November 5, 2015 by 11:00pm}
\end{center}
\normalsize

\begin{enumerate}  

%% ===============================================================
% QUESTION #1 : Basilisk
%% =============================================================

\item (20 pts) Answer  the questions below based on  the Basilisk
  algorithm for  semantic class induction, using the seed words
 for three semantic categories ({\sc animal}, {\sc vehicle},
  and {\sc instrument}) and pattern data shown below. 
 The table of pattern data includes four patterns and the nouns that
 each pattern extracted  in an imaginary corpus.  For logarithms, use
 log base 2. 

\begin{quote}
\hspace*{.5in} {\bf Animal Seeds:} jaguar, shark, walrus, zebra \\
\hspace*{.5in} {\bf Instrument Seeds:} bass, flute, horn, violin \\
\hspace*{.5in} {\bf Vehicle Seeds:} altima, impala, mustang, prius \\
\end{quote}

\begin{center}
\begin{tabular}{ll} \hline
\textbf{Pattern} & \textbf{Extracted Nouns} \\ \hline
patternA & bass, bronco, dog, impala, jaguar, mustang, shark, tiger, zebra \\ 
patternB & beetle, bronco, horn, jaguar, mustang, prius, tire \\ 
patternC & bass, clarinet, flute, music, piano, sound, trumpet, violin \\ 
patternD & accord, altima, bronco, jaguar, legacy, prius, sound \\ \hline 
\end{tabular}
\end{center}

\vspace*{.2in}

\begin{enumerate}
\item Compute RlogF(patternA) for the {\sc animal} category.\\ 
Answer : RlogF(patternA) = $\frac{3}{9}.\log_2 (3) = \frac{1}{3}.(1.585) = 0.5283$

\item Compute RlogF(patternA) for the {\sc vehicle} category. \\
Answer : RlogF(patternA) = $\frac{2}{9}.\log_2 (2) = \frac{2}{9}.(1) = \frac{2}{9} =  0.22$
\item Compute RlogF(patternA) for the {\sc instrument} category. \\
Answer : RlogF(patternA) = $\frac{1}{9}.\log_2 (1) = \frac{1}{9}.(0) = 0$

\item Compute RlogF(patternB) for the {\sc animal} category. \\
Answer : RlogF(patternB) = $\frac{1}{7}.\log_2 (1) = \frac{1}{7}.(0) = 0$

\item Compute RlogF(patternB) for the {\sc vehicle} category. \\
Answer : RlogF(patternB) = $\frac{2}{7}.\log_2 (2) = \frac{2}{7}.(1) = 0.2857$

\item Compute RlogF(patternB) for the {\sc instrument} category. \\
Answer : RlogF(patternB) = $\frac{1}{7}.\log_2 (1) = \frac{1}{7}.(0) = 0$

\item Compute AvgLog(``bronco'') for the {\sc animal} category. \\
Answer : Avglog(``bronco'') = $\frac{\log_2 (3+1) + \log_2 (1+1) + \log_2 (1+1)}{3} \\= \frac{\log_2 (4) + \log_2 (2)+ \log_2 (2)}{3} = \frac{2+1+1}{3} = \frac{4}{3} = 1.333$

\item Compute AvgLog(``bronco'') for the {\sc vehicle} category. \\
Answer : Avglog(``bronco'') = $\frac{\log_2 (2+1) + \log_2 (2+1) + \log_2 (2+1)}{3} \\= \frac{\log_2 (3) + \log_2 (3) + \log_2 (3)}{3} = \frac{1.585+1.585+1.585}{3} = \frac{4.755}{3} = 1.585$
\item Compute AvgLog(``sound'') for the {\sc instrument} category. \\
Answer : Avglog(``sound'') = $\frac{ \log_2 (3+1) + \log_2 (0+1)}{2} \\= \frac{\log_2 (4) + \log_2 (1)}{2} = \frac{2+0}{2} = 1$
\item Compute AvgLog(``sound'') for the {\sc vehicle} category.\\ 
Answer : Avglog(``sound'') = $\frac{ \log_2 (0+1) + \log_2 (2+1)}{2} \\= \frac{\log_2 (1) + \log_2 (3)}{2} = \frac{0+1.585}{2} = \frac{1.585}{2} = 0.7925$
\end{enumerate}


\newpage
%% ===============================================================
% QUESTION #2 : Distributional Similarity
%% =============================================================

\item (16 pts) Consider the following context vectors:

\begin{quote}
\hspace*{.5in} $word1$ : $<$5 3 4 0 7$>$ \\
\hspace*{.5in} $word2$ : $<$6 8 0 2 1$>$  \\
\hspace*{.5in} $word3$ : $<$2 7 1 5 4$>$ 
\end{quote}

Compute the similarity scores below using the word
vectors above. Please leave your answers in fractional form!

\begin{enumerate}
\item Similarity($word1$, $word2$) using Manhattan Distance.  \\ Answer: 18\\

\item Similarity($word2$, $word3$) using Manhattan Distance.  \\ Answer: 12 \\

\item Similarity($word1$, $word2$) using Jaccard Similarity.  \\ Answer: 1/3 \\

\item Similarity($word2$, $word3$) using Jaccard Similarity.  \\ Answer: 1/2 \\

\item Similarity($word1$, $word2$) using Cosine Similarity.  \\ Answer: $\frac{61}{\sqrt{99}\sqrt{105}} = \frac{61}{(9.95).(10.25)} = \frac{61}{101.9875} = 0.598 $ \\

\item Similarity($word2$, $word3$) using Cosine Similarity.  \\ Answer: $\frac{82}{\sqrt{105}\sqrt{95}} = \frac{82}{(10.25).(9.75)} = \frac{82}{99.9375} = 0.821$ \\
\end{enumerate}


\newpage
%% =================================================================
%% QUESTION #3 : Collins & Singer Named Entity Recognition
%% =================================================================
\item (32 pts) This question relates to the Collins \& Singer
  bootstrapping method for named entity recognition.  The predicate
  Contains($w$) is satisfied if a sequence of words includes the word
  $w$. TABLE 1 shows contains NP/Context pairs extracted
  from an imaginary text corpus, with their labels for two classes:
  {\sc human (hum)} and {\sc location (loc)}. 

\begin{center}
\textbf{TABLE 1} \\
~ \\
\begin{tabular}{|lll|} \hline
\textbf{NP}  & \textbf{CONTEXT} & \textbf{CLASS} \\ \hline
michael jordan & nike spokesman & {\sc hum} \\
jordan south & nike client & {\sc hum} \\
jeff jordan & circuit city ceo  & {\sc hum} \\ 
michael jordan & nike ceo  & {\sc hum} \\ 
jeff west & ceo  & {\sc hum} \\ 
south salt lake & mall in & {\sc loc} \\
jordan & country & {\sc loc} \\
south jordan & city & {\sc loc} \\ 
salt lake & capital city & {\sc loc} \\
west jordan & mall in & {\sc loc} \\ 
\hline
\end{tabular}
\end{center}

\begin{enumerate}
\item (14 pts) Using the Contains($w$) predicate, list all of the 
{\bf NP Rules} that would be generated from the NPs in TABLE
  1 and compute the probabilities P({\sc hum}) and P({\sc loc}) for each
  rule. \textbf{Leave the probabilities in   fractional form!}  \\ 

   \begin{center} 
    \begin{tabular}{lll} 
   \textbf{NP Rule~~~~~~~~~~~~~~~~~~~~~~~~~~~~~} &
   \textbf{P({\sc hum)~~~~~~~}} & \textbf{P({\sc loc})} \\ \hline
        Michael & 2/2 & 0/2 \\
        Jordan & 4/7 & 3/7 \\
        South & 1/3 & 2/3 \\ 
        Jeff & 2/2 & 0/2 \\ 
        West & 1/2 & 1/2 \\ 
        Salt & 0/2 & 2/2 \\ 
        Lake & 0/2 & 2/2 \\ \hline\\
   \end{tabular}
   \end{center}
 \vspace*{.2in}


\newpage
\item (6 pts) List the NP rules that would be produced by selecting
  rules from the table above that would have a probability $>$ .60. Then apply these NP rules to the
  instances in TABLE 2 below (i.e., fill in  TABLE 2 with the class
  label that would be assigned to each instance). If no class
  would be assigned, simply put {\it none}. \\
Answer: The NP rules that have a probability $> .60$ are:\\
\begin{center}
 \begin{tabular}{|llll|}\hline
 if Contains(Michael) & $->$ & {\sc hum} & $1$\\  
 if Contains(Jeff) & $->$ & {\sc hum} & $1$\\ 
 if Contains(Salt) & $->$ & {\sc loc} & $1$\\ 
 if Contains(Lake) & $->$ & {\sc loc} & $1$\\
 if Contains(South) & $->$ & {\sc loc} & $0.67$ \\  \hline 
 \end{tabular}
 \end{center}
 \begin{center}
 \textbf{TABLE 2} \\  ~ \\
 \begin{tabular}{|lll|} \hline
 \textbf{NP}  & \textbf{CONTEXT} & \textbf{~~~~~~CLASS~~~~~} \\ \hline
 ken jordan & south lake corp & {~~~~~~~~\it none~~~~~~~~} \\  
 jeff jones &  west corp ceo & {~~~~~~~~\sc hum~~~~~~~~} \\ 
 adam west & salt lake  & {~~~~~~~~\it none~~~~~~~~} \\ 
 michael south  & ceo  & {~~~~~~~~\sc hum~~~~~~~~} \\ 
 south salton sea   & lake & {~~~~~~~~\sc loc~~~~~~~~} \\ 
 mirror lake & west   & {~~~~~~~~\sc loc~~~~~~~~} \\ \hline 
 \end{tabular}
 \end{center}
 \vspace*{.2in}

\item (12 pts) Using the Contains($w$) predicate, list all of the 
\textbf{Context Rules} that would be generated from the CONTEXTS in TABLE
  2 and compute the probabilities P({\sc hum}) and P({\sc loc}) for
  each rule (using the class labels that you assigned). 
\textbf{Leave the     probabilities in fractional form!}  \\

   \begin{center} 
    \begin{tabular}{lll} 
   \textbf{Context Rule~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~} &
   \textbf{P({\sc hum)~~~~~~~~~}} & \textbf{P({\sc loc})} \\ \hline
       south & - & - \\ 
       lake & 0/1 & 1/1 \\
       corp & 1/1 & 0/1 \\
       west & 1/2 & 1/2 \\
       ceo & 2/2 & 0/2 \\
       salt & - & - \\ \hline
       
   \end{tabular}
   \end{center}
 \vspace*{.2in}


\end{enumerate}

\newpage
%% ===============================================================
% QUESTION #4 : Semantic Roles
%% =============================================================

\item (26 pts) For each sentence below, label the head noun of each noun
  phrase (NP) with the thematic role that is most appropriate based on its
  semantic relationship with the main verb.  

\begin{enumerate}

\item Jenny sold a diamond necklace with a matching diamond bracelet to
  the actress.  \\ 
  Jenny  - Agent  \\
  necklace - Theme \\
  bracelet - Co-Theme \\
  actress -  Recipient\\

\item The man repaired the broken pipe with duct tape.  \\ 
man - Agent\\
pipe - Theme\\
tape -  Instrument\\

\item Susan lent Thomas her car on Monday.  \\ 
Susan   - Agent\\
Thomas - Recipient\\
car - Theme\\
Monday - Time \\

\item The musician played his trumpet for President Obama.   \\ 
musician - Agent \\
trumpet - Theme\\
Obama -  Beneficiary\\

\item The girl is hiking with her sister from Logan to Pocatello. \\ 
girl - Agent\\
sister - Co-Agent\\
Logan - From-Loc\\
Pocatello -  To-Loc\\

\item The boat sank with its ten passengers.  \\ 
boat - Theme\\
passengers -  Co-Theme\\

\item The bird flew along the mountain trail with its powerful
  wings. \\
  bird - Agent\\
  trail - Path-Loc\\
  wings -  Instrument\\

\item The Disney movie was watched by three parents with their
  children. \\ 
  movie - Theme\\
  parents - Agent\\
  children -  Co-Agent\\

\end{enumerate}


\newpage
%% ===============================================================
% QUESTION #5 : PMI
%% =============================================================

\item (6 pts) Imagine that you have 5 tiny documents that each
  contains just a few words, which are shown below. 

\begin{quote}
\textbf{DOC \#1:} {\it natural} {\it language} {\it processing} {\it rules} \\
\textbf{DOC \#2:} {\it natural} {\it food} {\it book} \\
\textbf{DOC \#3:} {\it natural} {\it gas} \\
\textbf{DOC \#4:} {\it natural} {\it language} {\it book} \\
\textbf{DOC \#5:} {\it language} {\it rules} {\it book}
\end{quote}

Using these documents, compute the Pointwise Mutual
Information (PMI) values below. Each probability P($x$) should be the likelihood of
$x$ occurring in a document. For example, P({\it food}) means the probability
that a document will contain the word {\it food}. You must fill in the
equation as well as show the final value.  \\ ~ \\

\begin{enumerate}
\item PMI({\it language},{\it rules}) \\ Answer : $\log_2\left(\frac{\mathcal{P}(f,w)}{\mathcal{P}(f).\mathcal{P}(w)}\right) = \log_2\left(\frac{\mathcal{P}({\it language},{\it rules})}{\mathcal{P}({\it language}).\mathcal{P}({\it rules})}\right) = \log_2\left(\frac{2/5}{(3/5).(2/5)}\right) = \log_2\left(\frac{2/5}{6/25}\right) = \log_2\left(\frac{5}{3}\right) = \log_2 (5) - \log_2 (3) = 2.322 - 1.585 = 0.737$ \\

\item PMI({\it natural},{\it book}) \\ Answer : $\log_2\left(\frac{\mathcal{P}(f,w)}{\mathcal{P}(f).\mathcal{P}(w)}\right) = \log_2\left(\frac{\mathcal{P}({\it natural},{\it book})}{\mathcal{P}({\it natural}).\mathcal{P}({\it book})}\right) = \log_2\left(\frac{2/5}{(4/5).(3/5)}\right) = \log_2\left(\frac{2/5}{12/25}\right) = \log_2\left(\frac{5}{6}\right) = \log_2 (5) - \log_2 (6) = 2.322 - 2.585 = -0.263$ \\

\end{enumerate}



\end{enumerate}  % END OF WRITTEN QUESTIONS



\newpage
\hspace*{1.5in}  {\bf ELECTRONIC SUBMISSION INSTRUCTIONS \\
\hspace*{1.5in} (a.k.a. ``What to turn in and   how to do it'')} 

{\bf Your written assignment \underline{must} be in .pdf format.} Please do not turn in
.doc or .docx files ... convert them to .pdf format before submitting them!

To submit this assignment, the CADE provides a web-based
facility for electronic handin, which can be found here:
\begin{center}
https://webhandin.eng.utah.edu/
\end{center}
Or you can log in to any of the CADE machines and issue the command:
\begin{center}
handin cs5340 written3 $<$filename$>$
\end{center}
Please name your file: YourName-written3.pdf (e.g., EllenRiloff-written3.pdf)\\


\vspace*{.2in}
\hrule
HELPFUL HINT: you can get a listing of the files  that you've already
turned in via electronic submission by using the `handin' command
without giving it a filename. For  example:
\begin{center}
handin cs5340 written3
\end{center}
will list all of the files that you've turned in thus far. 
If you submit a new file with the same name as a previous file, the
new file will overwrite the old one.  


\end{document}


